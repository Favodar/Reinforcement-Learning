{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration Algorithm\n",
    "\n",
    "Table of contents  \n",
    "* [Abstract](#abstract)   \n",
    "* [The environment](#the-environment)  \n",
    "* [Imports](#imports)  \n",
    "* [Measuring execution time](#measuring-execution-time)  \n",
    "* [The Policy Evaluation Function](#the-policy-evaluation-function)  \n",
    "* [The Policy Improvement Function](#the-policy-improvement-function)  \n",
    "* [Interpreting the Output](#interpreting-the-output)  \n",
    "\n",
    "  \n",
    "### Abstract <a name=\"abstract\"></a>\n",
    "\n",
    "1. Generate random policy\n",
    "2. Generate random value function\n",
    "3. ???\n",
    "4. Profit\n",
    "\n",
    "Policy iteration is a technique for finding an optimal* policy in a given, fully known environment. It is neither fast nor efficient, and the practical applications are therefore limited. However, it is guaranteed to find an optimal policy in finite time, and serves as an important starting point for many reinforcement learning algorithms.\n",
    "\n",
    "The algorithm generates a random policy and evaluates it by calculating the value function (with the help of a policy evaluation algorithm).\n",
    "It then improves the random policy by changing its guidelines according to the newly obtainted value function (choosing actions which lead to the states with the maximum values). This does not yield an optimal policy yet, since the value function is based on a random, (probably**) suboptimal policy.  \n",
    "It then again submits the new, improved policy for review to the evaluation algorithm, receiving the new value function. Again, the policy is updated based on the value function. This process is repeated (iteration!) until there's nothing left to improve - optimality is achieved!\n",
    "\n",
    "In this example, we will try to find the optimal policy for navigating Jack's car rental problem, an excercise provided in the standard reference of Reinforcement Learning, [\"Reinforcement Learning: An Introduction\"](http://incompleteideas.net/book/bookdraft2017nov5.pdf) by Sutton and Barto.\n",
    "The policy iteration code was copied (and slightly modified) from [Denny Britz' GitHub](https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Policy%20Iteration%20Solution.ipynb), but is applied to another problem in this tutorial. \n",
    "_________\n",
    "<sup id=\"fn1\">\n",
    "*why not THE optimal policy? Because technically, there could be several different policies that yield the same reward, and if there's no *better* policy, then multiple optimal policies exist.  \n",
    "</sup>\n",
    "<br>\n",
    "<sup id=\"fn2\">\n",
    "**you could, by chance, have guessed an optimal policy. Gladstone Gander's policy iteration algorithm ends here.\n",
    "</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment <a name=\"the-environment\"></a>\n",
    "\n",
    "You don't need to know the details of the environment in order to understand policy iteration. You can find the full description [here](http://incompleteideas.net/book/bookdraft2017nov5.pdf).  \n",
    "  \n",
    "In brief, Jack runs two car rentals. He earns money (reward) when someone requests a car, and the location is able to provide a car. If he perceives an imbalance between the two locations, he has the possibility to move cars from one spot to another at a minor cost while the business is closed at night.\n",
    "But because the requests are randomly and unequally distributed over both locations, it is not obvious how this balancing should be done.  \n",
    "This is where reinforcement learning comes in; if we see earned money as reward, and expenditures as punishment (or negative reward), we are able to calculate guidelines for balancing the count of cars that maximize expected reward (which in this case equals maximizing expected income)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports <a name=\"imports\"></a>\n",
    "First, we need to import some libraries.  \n",
    "`time` is imported to measure run times.  \n",
    "`numpy` makes some numerical operations easier.  \n",
    "`CarRentalEnvironment` provides the environment to which we want to apply our policy iteration algorithm.  \n",
    "`matplotlib.pyplot` will be used to visualize our purely numerical results.  \n",
    "  \n",
    "This page is a Jupyter Notebook that allows you to run and also modify the code that's written in code cells. Go ahead and execute the imports below by either clicking on the \"Run this cell\"-button, or by clicking in the cell and pressing shift + enter. As I said, you are able to modify the code, but I recommend not messing with the imports. You will get to play later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "import numpy as np\n",
    "import CarRentalEnvironment as jack\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring execution time <a name=\"measuring-execution-time\"></a>\n",
    "Let us begin by storing the time at the start to keep track of the time it takes to execute the code.\n",
    "\n",
    "\n",
    "Next, we initialize the environment. This might take a while, since a transition list of all possible transitions is generated. Because in our example - Jack's Car Rental - there is at least a tiny chance to land in almost any state, regardless of the action we take, the transition list is huge (relative to the number of states). It contains every possible combination of starting state, target state, reward and action.  \n",
    "When this is done, we measure the time again to find out how long this step took. This is relevant if we want to optimize our algorithms, because we need to know how long it took to generate the transition list.\n",
    "\n",
    "jack.env() sets up the environment and return an object of the environment class.  \n",
    "Let's have a look at the environment class as defined in the imported CarRentalEnvironment. Pay special attention to the attributes; they will be of key importance in the next steps!\n",
    "\n",
    "    class environment:\n",
    "        \"\"\"\n",
    "        Complete information environment class with attributes that fully define the\n",
    "        environment. The environment is a Markov decision process.\n",
    "        \n",
    "        env() should be used to initialize an environment object according to the\n",
    "        Jack's Car Rental excercise.\n",
    "        \n",
    "        Attributes:\n",
    "            P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            nS is a number of states in the environment. \n",
    "            nA is a number of actions in the environment.\n",
    "            shape[] is optional for visualization purposes. It is a list of \n",
    "            integers representing the edge lengths of a matrix that contains the \n",
    "            states. E.g. if the states are best mapped on a 21x21 matrix, the list \n",
    "            should state [21, 21]\n",
    "        \"\"\"\n",
    "        def __init__(self, P, nS, nA):\n",
    "            self.P = P\n",
    "            self.nS = nS\n",
    "            self.nA = nA\n",
    "            self.shape = [nrOfStates, nrOfStates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"start!\")\n",
    "\n",
    "\n",
    "#env = GridworldEnv()\n",
    "env = jack.env()\n",
    "\n",
    "env_time = time.time()\n",
    "iterationCount = 1\n",
    "print(\"Run time of transition list generation = {}s\".format(env_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy Evaluation Function <a name=\"the-policy-evaluation-function\"></a>\n",
    "Now we can start writing our algorithm. At first we need to implement a policy evaluation function, which is a necessary tool for a policy iteration algorithm.\n",
    "The policy evaluation function is supposed to evaluate a policy that it is given (as an argument), by producing a value function (or utility function). The value function has the form of a vector, with values corresponding to every state. Since the values represent the expected reward (utility) of a state, not of a transition, the vector is relatively small, containing \"only\" as many values as there are states. This is in contrast to the number of state-action-reward-state combinations, which is much higher.\n",
    "\n",
    "#### Initializing the Value Function\n",
    "\n",
    "At first we initialize a value function filled with zeros, whose values are iteratively improved until they become the true values. It doesn't matter with which values you start, they will always converge to the correct solution. In theory, you can speed up the process if you know how the final values roughly look like - by starting with values which are close to that.\n",
    "For example, if you know the final values will mostly be around 20, you could initialize the value function with 20s at each point to achieve fewer iterations. It will make almost no difference in execution time though, because the final value is approximated quadratically.\n",
    "But go ahead and try it out yourself by changing the `0` in `V = np.full(env.nS, 0, float)`!\n",
    "You might need to pick extreme values to get a noticable difference in speed. Note that the same code will have slight variations in execution time when run several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Policy Evaluation Exercise!\n",
    "\n",
    "def policy_eval(policy, env, discount_factor, theta=0.00001): #original theta value theta=0.00001\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Some visualization of the policy iteration steps\n",
    "    global iterationCount\n",
    "    print(\"evaluating policy iteration \" + str(iterationCount) + \", discount factor = \" + str(discount_factor) + \", theta = \" + str(theta))\n",
    "    print(\"Policy:\")    \n",
    "    visualizePretty(np.reshape(np.argmax(policy, axis=1)-5, env.shape), \"policy \" + str(iterationCount))\n",
    "    print(\"\")\n",
    "    iterationCount += 1\n",
    "    ##\n",
    "    \n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.full(env.nS, 0, float) # was originally: V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                #for tupel in env.P[s][a]:\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                        #for  prob, next_state, reward, done in tupel:\n",
    "                        # Calculate the expected value\n",
    "                        v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The For-Each Loop Over `env.P`\n",
    "`env.P[s][a]` is a list of all possible transitions from state $s$, under the condition that action $a$ is selected. The list contains target states (possible next states) with their respective rewards and probabilities.\n",
    "This for-each loop calculates the value (expected future reward) of a state under the current policy by summing up the possible rewards which are discounted by the probability of  \n",
    "* the transition itself  \n",
    "* choosing the action under the given policy  \n",
    "\n",
    "and adding the value of the next state, discounted by the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    #for  prob, next_state, reward, done in tupel:\n",
    "                        # Calculate the expected value\n",
    "                        v += action_prob * prob * (reward + discount_factor * V[next_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Condition For The Outer Loop\n",
    "So why is there a `while True:` loop? Shouldn't the loop end once the for-each loop has iterated through every element and has therefore considered all transitions?  \n",
    "The answer to why the latter doesn't work is that when we calculate the value of a state, we need to take into account the value of the next states. We do that by simply adding the value that is given by our value function. But the values of our value function are arbitrary and very likely not correct - we initialized them to be 0 or another guessed value. This means after our first loop through all transitions, the values are all based on a (probably) wrong value function.  \n",
    "We did, however, add some truth to our value function: we considered the (correct) rewards and transition probabilities. This means our value function is not completely arbitrary anymore, it gained some \"knowledge\" about the true values. By circling through this process often enough, we get closer and closer to the correct values in our value function. This is why we have the \"while\" loop in the beginning.  \n",
    "We know that we have the correct values in our value function when an update cycle completes without any changes.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>\n",
    "\n",
    "______\n",
    "<sup id=\"fn1\">1. For proof that convergence is guaranteed with any starting values, see Theorem 3.6 in [\"From ants to safe Artificial Intelligence: Reinforcement Learning\" (Lang, 2018)](https://github.com/Favodar/Reinforcement-Learning/blob/master/Reinforcement%20Learning%2C%20Part%201.pdf)<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy Improvement Function <a name=\"the-policy-improvement-function\"></a>\n",
    "\n",
    "We will now get to the core of the policy iteration algorithm: the policy_improvement function improves a policy until it becomes an optimal policy for maximizing utility in a given environment.\n",
    "\n",
    "#### Initializing the policy\n",
    "\n",
    "As a first step, we generate a random policy, similar to  what we did earlier when we set up a random value function `policy = np.ones([env.nS, env.nA]) / env.nA`\n",
    "  \n",
    "#### Evaluating the policy\n",
    "\n",
    "Next, we let our evaluation function do the job of evaluating this policy. How *value*able are the states if we act according to this policy (in terms of long term expected reward)? This doesn't tell us yet how good our policy is. If we have a utility of, say, 914 in one state, is that good? We don't know, because it's all relative. But we do have a reference now. Future versions of our policy can be measured against this, and that's exactly what we're gonna do!  \n",
    "\n",
    "#### One step lookahead\n",
    "\n",
    "What we'll do next to improve our policy is to calculate the values of the available actions. So instead of just looking at the state value (which we already did), we look at a state-action pair and calculate the long-term expected reward of this pair. We can do this by looking at where the action could potentially bring us, and summing up the values of those states, multiplied (meaning discounted) by the probability of getting there.  \n",
    "  \n",
    "For example, let's say we are in state $s_2$ and if we take action $a_3$, there's a 20% chance to get to state $s_8$, and $s_8$ has a value of $v(s_8)=10$. Then we start by multiplying $v(s_8)$ with the 20% chance:  \n",
    "$v(s_8)*p(s_8|s_2, a_3) = 10*0.2 = 2$  \n",
    "\n",
    "    for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "     return A\n",
    "\n",
    "So $2$ is the first value we have to keep in mind. But let's say action 3 also yields a 10% chance to arrive in state 20, and state 20 has an astounding value of 360. Then we need to add $360*0.1 = 36$ to what we already got, which is 2. If we keep doing this with every state that action 3 could bring us to, and add up all these values, then we have the value of the state action pair \"state 2 - action 3\" (under our current policy). Now if we do this for **all** the actions that are available in state 2 (you probably realize by now that this is a lot of calculations), we can chose the one with the highest value and put *this* has our action-recommendation in our policy. [I think some magic happens here before the next step]  \n",
    "Since our policy has now changed (improved!), we need to re-evaluate it to get our value-function up to date as well, and then we can rinse and repeat.  \n",
    "If we do this often enough, we will eventually get an optimal policy. How do we know we have an optimal policy? Same as with the policy evaluation function - if the update cycle completes without changes, we know we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, discount_factor, theta=0.00001, policy_eval_fn=policy_eval):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI environment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor, theta)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Output <a name=\"interpreting-the-output\"></a>\n",
    "\n",
    "#### Policy Probability Distribution\n",
    "\n",
    "Each row of the policy probability distribution represents a state, while the columns represent the actions.\n",
    "The numbers signify the probability of taking the action in that state under the policy, with a value between 0 and 1.  \n",
    "For example, a `1` in position $(2,6)$ means a 100% percent chance to take the 6th action in the 2nd state (which is the state where c1 = 0 and c2 = 1). The 6th action is the action \"0\" or \"move no cars\".\n",
    "\n",
    "#### Reshaped Policy\n",
    "\n",
    "The reshaped policy shapes the policy into a matrix where the x-axis represents the number of cars at c1 and the _inverted_ y-axis signifies c2.\n",
    "The numbers in the matrix are the action with the highest probability at that state, under the final policy.\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The value function matrix has the same structure as the reshaped policy, but the numbers signify the value (expected future reward, or utility) of that state, if the final policy is followed. The value function is not a policy as it doesn't contain instructions. It tells you how valuable a state is *if* you choose optimal actions from there on, but it doesn't tell you what those actions are (assuming it is the value function of the optimal policy, as it is the case here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePretty(array, title=\"title goes here\"):\n",
    "    ## \n",
    "    dim_1, dim_2 = array.shape\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    im = ax.imshow(array)\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(dim_1):\n",
    "        for j in range(dim_2):\n",
    "            text = ax.text(j, i, array[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22}) #this just sets the font size for the visualizing plots\n",
    "\n",
    "policy, v = policy_improvement(env, 0.9, 0.00001)\n",
    "\n",
    "iteration_time = time.time()\n",
    "print(\"Run time of policy iteration = {} sec\".format(iteration_time-env_time))\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Policy (-5 = move 5 cars from B to A, 0 = move no cars, 5 = move 5 cars from A to B):\")\n",
    "#print(\"(not implemented)\")\n",
    "print(np.reshape(np.argmax(policy, axis=1)-5, env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"visualizePretty:\")\n",
    "visualizePretty(np.reshape(np.argmax(policy, axis=1)-5, env.shape), \"final policy\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "#print(\"(not implemented)\")\n",
    "list3 = []\n",
    "for number in v:    \n",
    "    list3.append(round(number, 3))\n",
    "#print(\"(not implemented)\")\n",
    "print(str(np.reshape(list3, env.shape)).replace(\"'\", \"\"))\n",
    "print(\"\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 5})\n",
    "\n",
    "print(\"visualizePretty:\")\n",
    "visualizePretty(np.reshape(v, env.shape), \"value function\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Run time total = {} sec\".format(end_time - start_time))\n",
    "print(\"Run time of transition list generation = {} sec\".format(env_time-start_time))\n",
    "print(\"Run time of policy iteration = {} sec\".format(iteration_time-env_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, convergence is *fast*. While the initial, arbitrary policy always recommended the action \"-5\", the first improvement (policy 2) is already very similar to the final policy, with only minor deviations. Of course, lots of operations take place between every iteration of the policy, but I still find it fascinating how a value function based on an awfully bad policy (\"move 5 cars from A to B, regardless of the current distribution of cars\") can guide us to an almost optimal policy in just one iteration.\n",
    "\n",
    "If you haven't done so already, you can play around with the discount rate and the theta value in `policy, v = policy_improvement(env, 0.9, 0.00001)`. You may notice that changing the discount rate doesn't change much of the result. Can you figure out why?  \n",
    "*Hint: think about what the discount rate actually does, and how optimizing for short-term reward and long-term reward would differ in our environment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

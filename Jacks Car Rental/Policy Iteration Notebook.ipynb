{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration Algorithm\n",
    "  \n",
    "[Abstract](#abstract)\n",
    "[The policy evaluation function](#the-policy-evaluation-function)\n",
    "  \n",
    "### Abstract\n",
    "\n",
    "1. Generate random policy\n",
    "2. Generate random value function\n",
    "3. ???\n",
    "4. Profit\n",
    "\n",
    "Policy iteration is a technique for finding an optimal* policy in a given, fully known environment. It is neither fast nor efficient, and the practical applications are therefore limited. However, it is guaranteed to find an optimal policy in finite (?) time, and serves as an important starting point for many reinforcement learning algorithms.\n",
    "\n",
    "The algorithm generates a random policy and evaluates it by calculating the value function (with the help of a policy evaluation algorithm).\n",
    "It then improves the random policy by changing its guidelines according to the newly obtainted value function (choosing actions which lead to the states with the maximum values). This does not yield an optimal policy yet, since the value function is based on a random, (probably**) suboptimal policy.  \n",
    "It then again submits the new, improved policy for review to the evaluation algorithm, receiving the new value function. Again, the policy is updated based on the value function. This process is repeated (iteration!) until there's nothing left to improve - optimality is achieved!\n",
    "\n",
    "In this example, we will try to find the optimal policy for navigating Jack's car rental problem, an excercise provided in the standard reference of Reinforcement Learning, [\"Reinforcement Learning: An Introduction\"](http://incompleteideas.net/book/bookdraft2017nov5.pdf) by Sutton and Barto.\n",
    "_________\n",
    "<sup id=\"fn1\">\n",
    "\\*why not *the* optimal policy? Because technically, there could be several different policies that yield the same reward, and if there's no *better* policy, then multiple optimal policies exist.  \n",
    "\\**you could, by chance, have guessed an optimal policy. Gladstone Gander's policy iteration algorithm ends here.\n",
    "</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment\n",
    "\n",
    "You don't need to know the details of the environment in order to understand policy iteration. You can find the full description [here](http://incompleteideas.net/book/bookdraft2017nov5.pdf).  \n",
    "  \n",
    "In brief, Jack runs two car rentals. He earns money (reward) when someone requests a car, and the location is able to provide a car. If he perceives an imbalance between the two locations, he has the possibility to move cars from one spot to another at a minor cost while the business is closed at night.\n",
    "But because the requests are randomly and unequally distributed over both locations, it is not obvious how this balancing should be done.  \n",
    "This is where reinforcement learning comes in; if we see earned money as reward, and expenditures as punishment (or negative reward), we are able to calculate guidelines for balancing the count of cars that maximize expected reward (which in this case equals maximizing expected income)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "First, we need to import some libraries.  \n",
    "`time` is imported to measure run times.  \n",
    "`numpy` makes some numerical operations easier.  \n",
    "`CarRentalEnvironment` provides the environment to which we want to apply our policy iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import CarRentalEnvironment as jack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring execution time\n",
    "Let us begin by storing the time at the start to keep track of the time it takes to execute the code.\n",
    "\n",
    "\n",
    "Next, we initialize the environment. This might take a while, since a transition list of all possible transitions is generated. Because in our example - Jack's Car Rental - there is at least a tiny chance to land in almost any state, regardless of the action we take, the transition list is huge: it contains every possible combination of starting state, target state, reward and action.  \n",
    "When this is done, we measure the time again to find out how long this step took. This is relevant if we want to optimize our algorithms, because we need to know how long it took to generate the transition list.\n",
    "\n",
    "> jack.env erklären, car rental problem evtl. erklären\n",
    "\n",
    "jack.env() sets up the environment and return an object of the environment class.  \n",
    "Let's have a look at the environment class as defined in the imported CarRentalEnvironment. Pay special attention to the attributes; they will be of key importance in the next steps!\n",
    "\n",
    "    class environment:\n",
    "        \"\"\"\n",
    "        Complete information environment class with attributes that fully define the\n",
    "        environment. The environment is a Markov decision process.\n",
    "        \n",
    "        env() should be used to initialize an environment object according to the\n",
    "        Jack's Car Rental excercise.\n",
    "        \n",
    "        Attributes:\n",
    "            P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            nS is a number of states in the environment. \n",
    "            nA is a number of actions in the environment.\n",
    "            shape[] is optional for visualization purposes. It is a list of \n",
    "            integers representing the edge lengths of a matrix that contains the \n",
    "            states. E.g. if the states are best mapped on a 21x21 matrix, the list \n",
    "            should state [21, 21]\n",
    "        \"\"\"\n",
    "        def __init__(self, P, nS, nA):\n",
    "            self.P = P\n",
    "            self.nS = nS\n",
    "            self.nA = nA\n",
    "            self.shape = [nrOfStates, nrOfStates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n",
      "Generating list of transition tuples, i: 0, j: 0\n",
      "Generating list of transition tuples, i: 0, j: 1\n",
      "Generating list of transition tuples, i: 0, j: 2\n",
      "Generating list of transition tuples, i: 0, j: 3\n",
      "Generating list of transition tuples, i: 0, j: 4\n",
      "Generating list of transition tuples, i: 0, j: 5\n",
      "Generating list of transition tuples, i: 1, j: 0\n",
      "Generating list of transition tuples, i: 1, j: 1\n",
      "Generating list of transition tuples, i: 1, j: 2\n",
      "Generating list of transition tuples, i: 1, j: 3\n",
      "Generating list of transition tuples, i: 1, j: 4\n",
      "Generating list of transition tuples, i: 1, j: 5\n",
      "Generating list of transition tuples, i: 2, j: 0\n",
      "Generating list of transition tuples, i: 2, j: 1\n",
      "Generating list of transition tuples, i: 2, j: 2\n",
      "Generating list of transition tuples, i: 2, j: 3\n",
      "Generating list of transition tuples, i: 2, j: 4\n",
      "Generating list of transition tuples, i: 2, j: 5\n",
      "Generating list of transition tuples, i: 3, j: 0\n",
      "Generating list of transition tuples, i: 3, j: 1\n",
      "Generating list of transition tuples, i: 3, j: 2\n",
      "Generating list of transition tuples, i: 3, j: 3\n",
      "Generating list of transition tuples, i: 3, j: 4\n",
      "Generating list of transition tuples, i: 3, j: 5\n",
      "Generating list of transition tuples, i: 4, j: 0\n",
      "Generating list of transition tuples, i: 4, j: 1\n",
      "Generating list of transition tuples, i: 4, j: 2\n",
      "Generating list of transition tuples, i: 4, j: 3\n",
      "Generating list of transition tuples, i: 4, j: 4\n",
      "Generating list of transition tuples, i: 4, j: 5\n",
      "Generating list of transition tuples, i: 5, j: 0\n",
      "Generating list of transition tuples, i: 5, j: 1\n",
      "Generating list of transition tuples, i: 5, j: 2\n",
      "Generating list of transition tuples, i: 5, j: 3\n",
      "Generating list of transition tuples, i: 5, j: 4\n",
      "Generating list of transition tuples, i: 5, j: 5\n",
      "Run time of transition list generation = 0.6103663444519043s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"start!\")\n",
    "\n",
    "\n",
    "#env = GridworldEnv()\n",
    "env = jack.env()\n",
    "\n",
    "env_time = time.time()\n",
    "print(\"Run time of transition list generation = {}s\".format(env_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy Evaluation Function\n",
    "Now we can start writing our algorithm. At first we need to implement a policy evaluation function, which is a necessary tool for a policy iteration algorithm.\n",
    "The policy evaluation function is supposed to evaluate a policy that it is given (as an argument), by producing a value function (or utility function). The value function has the form of a vector, with values corresponding to every state. Since the values represent the expected reward (utility) of a state, not of a transition, the vector is relatively small, containing \"only\" as many values as there are states. This is in contrast to the number of state-action-reward-state combinations, which is much higher.\n",
    "\n",
    "#### Initializing the Value Function\n",
    "\n",
    "At first we initialize a value function filled with zeros, whose values are iteratively improved until they become the true values. It doesn't matter with which values you start, they will always converge to the correct solution. In theory, you can speed up the process if you know how the final values roughly look like - by starting with values which are close to that.\n",
    "For example, if you know the final values will mostly be around 20, you could initialize the value function with 20s at each point to achieve fewer iterations. It will make almost no difference in execution time though, because the final value is approximated quadratically.\n",
    "But go ahead and try it out yourself by changing the `0` in `V = np.full(env.nS, 0, float)`!\n",
    "You might need to pick extreme values to get a noticable difference in speed. Note that the same code will have slight variations in execution time when run several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor, theta=0.00001): #original theta value theta=0.00001\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    print(\"evaluating policy, discount factor = \" + str(discount_factor) + \", theta = \" + str(theta))\n",
    "    print(\"Policy:\")\n",
    "    print(np.reshape(np.argmax(policy, axis=1)-5, env.shape))\n",
    "    print(\"\")\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.full(env.nS, 0, float)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                #for tupel in env.P[s][a]:\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    #for  prob, next_state, reward, done in tupel:\n",
    "                        # Calculate the expected value\n",
    "                        v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The For-Each Loop Over `env.P`\n",
    "`env.P[s][a]` is a list of all possible transitions from state $s$, under the condition that action $a$ is selected. The list contains target states (possible next states) with their respective rewards and probabilities.\n",
    "This for-each loop calculates the value (expected future reward) of a state under the current policy by summing up the possible rewards which are discounted by the probability of  \n",
    "* the transition itself  \n",
    "* choosing the action under the given policy  \n",
    "\n",
    "and adding the value of the next state, discounted by the discount factor.\n",
    "\n",
    "\n",
    "> ###### Change last sentence so that it is clear that the two discountings also act on the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    #for  prob, next_state, reward, done in tupel:\n",
    "                        # Calculate the expected value\n",
    "                        v += action_prob * prob * (reward + discount_factor * V[next_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Condition For The Outer Loop\n",
    "So why is there a `while True:` loop? Shouldn't the loop end once the for-each loop has iterated through every element and has therefore considered all transitions?  \n",
    "The answer to why the latter doesn't work is that when we calculate the value of a state, we need to take into account the value of the next states. We do that by simply adding the value that is given by our value function. But the values of our value function are arbitrary and very likely not correct - we initialized them to be 0 or another guessed value. This means after our first loop through all transitions, the values are all based on a (probably) wrong value function.  \n",
    "We did, however, add some truth to our value function: we considered the (correct) rewards and transition probabilities. This means our value function is not completely arbitrary anymore, it gained some \"knowledge\" about the true values. By circling through this process often enough, we get closer and closer to the correct values in our value function. This is why we have the \"while\" loop in the beginning.  \n",
    "We know that we have the correct values in our value function when an update cycle completes without any changes.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>\n",
    "\n",
    "______\n",
    "<sup id=\"fn1\">1. For proof that convergence is guaranteed with any starting values, see Theorem 3.6 in \"From ants to safe Artificial Intelligence: Reinforcement Learning\" (Lang, 2018)<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "  \n",
    "\n",
    "> Theorem 3.6 ist die Verallgemeinerung der Aussage, dass Policy Evaluation immer konvergiert, egal was der Start-Vector ist :D Darauf kannst du verweisen, wenn du willst :D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What does this mean? Refer to Leon's fully general proof.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy Improvement Function\n",
    "\n",
    "We will now get to the core of the policy iteration algorithm: the policy_improvement function improves a policy until it becomes an optimal policy for maximizing utility in a given environment.\n",
    "\n",
    "#### Initializing the policy\n",
    "\n",
    "As a first step, we generate a random policy, similar to  what we did earlier when we set up a random value function `policy = np.ones([env.nS, env.nA]) / env.nA`\n",
    "> (what shape is that exactly?)\n",
    "  \n",
    "#### Evaluating the policy\n",
    "\n",
    "Next, we let our evaluation function do the job of evaluating this policy. How *value*able are the states if we act according to this policy (in terms of long term expected reward)? This doesn't tell us yet how good our policy is. If we have a utility of, say, 914 in one state, is that good? We don't know, because it's all relative. But we do have a reference now. Future versions of our policy can be measured against this, and that's exactly what we're gonna do!  \n",
    "\n",
    "#### One step lookahead\n",
    "\n",
    "What we'll do next to improve our policy is to calculate the values of the available actions. So instead of just looking at the state value (which we already did), we look at a state-action pair and calculate the long-term expected reward of this pair. We can do this by looking at where the action could potentially bring us, and summing up the values of those states, multiplied (meaning discounted) by the probability of getting there.  \n",
    "  \n",
    "For example, let's say we are in state $s_2$ and if we take action $a_3$, there's a 20% chance to get to state $s_8$, and $s_8$ has a value of $v(s_8)=10$. Then we start by multiplying $v(s_8)$ with the 20% chance:  \n",
    "$v(s_8)*p(s_8|s_2, a_3) = 10*0.2 = 2$  \n",
    "> isnt that like incomplete? like, shouldnt there be a reward in the transition and stuff?  \n",
    "> cuz da code be lookin' like dis:\n",
    "> \n",
    "\n",
    "    for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "     return A\n",
    "\n",
    "So $2$ is the first value we have to keep in mind. But let's say action 3 also yields a 10% chance to arrive in state 20, and state 20 has an astounding value of 360. Then we need to add $360*0.1 = 36$ to what we already got, which is 2. If we keep doing this with every state that action 3 could bring us to, and add up all these values, then we have the value of the state action pair \"state 2 - action 3\" (under our current policy). Now if we do this for **all** the actions that are available in state 2 (you probably realize by now that this is a lot of calculations), we can chose the one with the highest value and put *this* has our action-recommendation in our policy. [I think some magic happens here before the next step]  \n",
    "Since our policy has now changed (improved!), we need to re-evaluate it to get our value-function up to date as well, and then we can rinse and repeat.  \n",
    "If we do this often enough, we will eventually get an optimal policy. How do we know we have an optimal policy? Same as with the policy evaluation function - if the update cycle completes without changes, we know we're done!\n",
    "  \n",
    "> algorithm generates a random policy and evaluates it by calculating the value function (with the help of a policy evaluation algorithm).\n",
    "It then improves the random policy by changing its guidelines according to the newly obtainted value function (choosing actions which lead to the states with the maximum values). This does not yield an optimal policy yet, since the value function is based on a random, (probably*) suboptimal policy.  \n",
    "It then again submits the new, improved policy for review to the evaluation algorithm, receiving the new value function. Again, the policy is updated based on the value function. This process is repeated (iteration!) until there's nothing left to improve - optimality is achieved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, discount_factor, theta=0.00001, policy_eval_fn=policy_eval):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI environment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor, theta)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Output\n",
    "\n",
    "#### Policy Probability Distribution\n",
    "\n",
    "Each row of the policy probability distribution represents a state, while the columns represent the actions.\n",
    "The numbers signify the probability of taking the action in that state under the policy, with a value between 0 and 1.  \n",
    "For example, a `1` in position $(2,6)$ means a 100% percent chance to take the 6th action in the 2nd state (which is the state where c1 = 0 and c2 = 1). The 6th action is the action \"0\" or \"move no cars\".\n",
    "\n",
    "#### Reshaped Policy\n",
    "\n",
    "The reshaped policy shapes the policy into a matrix where the x-axis represents the number of cars at c1 and the _inverted_ y-axis signifies c2.\n",
    "The numbers in the matrix are the action with the highest probability at that state, under the final policy.\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The value function matrix has the same structure as the reshaped policy, but the numbers signify the value (expected future reward, or utility) of that state, if the final policy is followed. The value function is not a policy as it doesn't contain instructions. It tells you how valuable a state is *if* you choose optimal actions from there on, but it doesn't tell you what those actions are (assuming it is the value function of the optimal policy, as it is the case here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize(number):\n",
    "    \"\"\"\n",
    "    A function that visualizes the value of a number with a character, ASCII style\n",
    "    \"\"\"\n",
    "    number = abs(number)\n",
    "    if number < 1:\n",
    "        return \" \"\n",
    "    if number < 2:\n",
    "        return \"-\"\n",
    "    if number < 3:\n",
    "        return \"=\"\n",
    "    if number < 4:\n",
    "        return \"≡\"\n",
    "    if number < 5:\n",
    "        return \"▒\"\n",
    "    if number < 6:\n",
    "        return \"▓\"\n",
    "    if number < 7:\n",
    "        return \"█\"\n",
    "    if number < 8:\n",
    "        return \"█\"\n",
    "    return \"█\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating policy, discount factor = 0.9, theta = 1e-05\n",
      "Policy:\n",
      "[[-5 -5 -5 -5 -5 -5]\n",
      " [-5 -5 -5 -5 -5 -5]\n",
      " [-5 -5 -5 -5 -5 -5]\n",
      " [-5 -5 -5 -5 -5 -5]\n",
      " [-5 -5 -5 -5 -5 -5]\n",
      " [-5 -5 -5 -5 -5 -5]]\n",
      "\n",
      "evaluating policy, discount factor = 0.9, theta = 1e-05\n",
      "Policy:\n",
      "[[ 0  0  1  1  2  2]\n",
      " [-1  0  0  1  1  2]\n",
      " [-1 -1  0  0  0  1]\n",
      " [-2 -1 -1  0  0  0]\n",
      " [-2 -2 -1  0  0  0]\n",
      " [-3 -2 -2 -1  0  0]]\n",
      "\n",
      "evaluating policy, discount factor = 0.9, theta = 1e-05\n",
      "Policy:\n",
      "[[ 0  0  1  1  2  2]\n",
      " [-1  0  0  1  1  2]\n",
      " [-1 -1  0  0  1  1]\n",
      " [-2 -1  0  0  0  0]\n",
      " [-2 -2 -1  0  0  0]\n",
      " [-3 -2 -1 -1  0  0]]\n",
      "\n",
      "Run time of policy iteration = 1.1864519119262695 sec\n",
      "Policy Probability Distribution:\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Policy (-5 = move 5 cars from B to A, 0 = move no cars, 5 = move 5 cars from A to B):\n",
      "[[ 0  0  1  1  2  2]\n",
      " [-1  0  0  1  1  2]\n",
      " [-1 -1  0  0  1  1]\n",
      " [-2 -1  0  0  0  0]\n",
      " [-2 -2 -1  0  0  0]\n",
      " [-3 -2 -1 -1  0  0]]\n",
      "\n",
      "Visualized Policy:\n",
      "[[    - - = =]\n",
      " [-     - - =]\n",
      " [- -     - -]\n",
      " [= -        ]\n",
      " [= = -      ]\n",
      " [≡ = - -    ]]\n",
      "\n",
      "Value Function:\n",
      "[3.93412814e-03 2.53373000e-02 1.17108651e-01 3.49703145e-01\n",
      " 8.85809480e-01 1.76184115e+00 2.26980595e-02 1.28617928e-01\n",
      " 3.89443678e-01 9.09274635e-01 1.80290923e+00 3.23198379e+00\n",
      " 1.17108652e-01 3.49703147e-01 9.97179552e-01 2.02758471e+00\n",
      " 3.29845546e+00 4.99476462e+00 3.39100187e-01 9.09274652e-01\n",
      " 1.80793400e+00 3.63742691e+00 5.70625814e+00 7.52880229e+00\n",
      " 8.85809527e-01 1.76184124e+00 3.29845557e+00 5.30839431e+00\n",
      " 8.24389372e+00 1.07740380e+01 1.72077324e+00 3.23198403e+00\n",
      " 4.91298589e+00 7.42596331e+00 1.04511080e+01 1.35014612e+01]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[4.0000e-03 2.5000e-02 1.1700e-01 3.5000e-01 8.8600e-01 1.7620e+00]\n",
      " [2.3000e-02 1.2900e-01 3.8900e-01 9.0900e-01 1.8030e+00 3.2320e+00]\n",
      " [1.1700e-01 3.5000e-01 9.9700e-01 2.0280e+00 3.2980e+00 4.9950e+00]\n",
      " [3.3900e-01 9.0900e-01 1.8080e+00 3.6370e+00 5.7060e+00 7.5290e+00]\n",
      " [8.8600e-01 1.7620e+00 3.2980e+00 5.3080e+00 8.2440e+00 1.0774e+01]\n",
      " [1.7210e+00 3.2320e+00 4.9130e+00 7.4260e+00 1.0451e+01 1.3501e+01]]\n",
      "\n",
      "Visualized Value Function:\n",
      "[[          -]\n",
      " [        - ≡]\n",
      " [      = ≡ ▒]\n",
      " [    - ≡ ▓ █]\n",
      " [  - ≡ ▓ █ █]\n",
      " [- ≡ ▒ █ █ █]]\n",
      "\n",
      "Run time total = 4.095125198364258 sec\n",
      "Run time of transition list generation = 2.9007110595703125 sec\n",
      "Run time of policy iteration = 1.1864519119262695 sec\n"
     ]
    }
   ],
   "source": [
    "#policy, v = policy_improvement(env, 0, 1)\n",
    "policy, v = policy_improvement(env, 0.9, 0.00001)\n",
    "\n",
    "iteration_time = time.time()\n",
    "print(\"Run time of policy iteration = {} sec\".format(iteration_time-env_time))\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Policy (-5 = move 5 cars from B to A, 0 = move no cars, 5 = move 5 cars from A to B):\")\n",
    "#print(\"(not implemented)\")\n",
    "print(np.reshape(np.argmax(policy, axis=1)-5, env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Visualized Policy:\")\n",
    "#print(\"(not implemented)\")\n",
    "list1 = []\n",
    "for number in np.argmax(policy, axis=1):    \n",
    "    list1.append(visualize(number-5))\n",
    "print(str(np.reshape(list1, env.shape)).replace(\"'\", \"\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "#print(\"(not implemented)\")\n",
    "list3 = []\n",
    "for number in v:    \n",
    "    list3.append(round(number, 3))\n",
    "#print(\"(not implemented)\")\n",
    "print(str(np.reshape(list3, env.shape)).replace(\"'\", \"\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Visualized Value Function:\")\n",
    "list2 = []\n",
    "for number in v:    \n",
    "    list2.append(visualize(number))\n",
    "#print(\"(not implemented)\")\n",
    "print(str(np.reshape(list2, env.shape)).replace(\"'\", \"\"))\n",
    "print(\"\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Run time total = {} sec\".format(end_time - start_time))\n",
    "print(\"Run time of transition list generation = {} sec\".format(env_time-start_time))\n",
    "print(\"Run time of policy iteration = {} sec\".format(iteration_time-env_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<!DOCTYPE html>
<html>

<head>

    <link rel="stylesheet" type="text/css" href="css/parallax.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="css/glassbuttons.css" />
    
    <meta charset="UTF-8" />


    <title>reinforcement learning</title>
    <link rel="stylesheet" type="text/css" href="css/normalize.css" />
<!--     <link rel="stylesheet" type="text/css" href="css/demo.css" /> -->
<!--     <link rel="stylesheet" type="text/css" href="css/component.css" /> -->

    <!-- Glass buttons as .less -->
 <!--    <link rel="stylesheet/less" type="text/css" href="glassemptyfile.less" /> -->
    <!-- <script src="js/modernizr.custom.js"></script> -->

    <!-- Browser-Side Less via CDN: -->
<!--     <script>less = { env: 'development'};</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/less.js/3.8.1/less.js"></script> -->

    <script src="js/jquery-3.3.1.js"></script>
    

    <!-- Page Transitions (not in use as of now): -->
    <script src="js/barba.js" type="text/javascript"></script>

    
</head>


<body>

        

    <div id="barba-wrapper">
    <div class="barba-container">
        

            
        <div class="parallax-1"></div>
                

        <div class="parallax_text" style="top: 40%; position: absolute; padding-top: 1em;">

                <h3><span title="You are the 10 000 000 000th visitor of this page!"> Congratulations!</span></h3> You just learned something with reinforcement learning (RL): click on the circles! <br>
                The basic concept of RL is simple: an <i>agent</i> is in a <i>state</i> and may perform different <i>actions</i>, which lead to new states. The actions are rewarded or punished (a punishment may also be called “negative reward”).
                Based on the received <i>rewards</i>, the agent learns to perform the desired actions. <br>
                Wait, but how do these terms map on the game with the squares and circles? Scroll down to find out!
            </div>

    <div class="parallax-1"></div>
    <div class="parallax-1"></div>

            








    <div class="parallax_text">


            In the preceding exercise, you were the "agent". The agent could also be described as the decision algorithm. What you saw on the screen, inside the browser window, would be called the current "state". The agent takes the current state as input, and based on this input, it chooses an "action". <br>
            <br>
            In this case, there were two possible "actions" in each state: clicking on the square, or clicking on the circle, which gave a reward in the case of the circle, and a punishment in the square-case.<br>
            So the state could be seen as the input, the agent as the algorithm, and the action as the output.
            <br>
            <img src="pictures/RL_agent_pseudocode.jpg" title="pseudocode of an RL agent" alt="pseudocode of an RL agent" style="padding:50px; width: 80%;">
            <br>
            <nav class="btn-bar nav-dark">
    
                <a class="btn btn-glass btn-info" onclick="reveal(this)" class="expand_button" id="-1">
                    <i class="fa fa-fw fa-lg fa-info " ></i> Notation
                </a>
            </nav>

        </div>

    <div class="overlay" id="overlay-1">
            <p>For the purpose of this introduction, I will use the following notation. This will only be relevant for some Detail Chapters, so you don&rsquo;t need it to understand the main lessons.</p>
            <p>s := state<br />a := action<br />r := reward<br />p(x|y) := probability of x, under the condition y</p>
            <p>Example: p(s&rsquo;,r|s,a) is the probability of transition to state s&rsquo; with reward r, when action a is taken in state s.<br />In other words, if you&rsquo;re in state s and execute action a, what&rsquo;s the probability of landing in state s&rsquo; with reward r? This probability is represented by p(s&rsquo;,r|s,a).</p>
    </div>

    <div class="parallax-3"></div>

    <div class="parallax_text">
        You learned, by means of reinforcement learning, to perform the optimal action for each state: clicking on the circle. The solution came to you so easily because
        <h3><span title="Or are they dancers?">humans are Reinforcement Learners</span></h3>
        (and because it was an easy exercise). Evolution came up with this concept and it worked extremely well, so well in fact that nowadays almost all animals have it.
        <br><br>
        <nav class="btn-bar nav-dark">

            <a class="btn btn-glass btn-info" onclick="reveal(this)" class="expand_button" id="0">
                <i class="fa fa-fw fa-lg fa-info " ></i> RL in animals
            </a>
        </nav>
    </div>
    

    <div class="overlay" id="overlay0">
            <p>Reinforcement learning in Animals</p>
            
            <p>Since &ldquo;reinforcement learning&rdquo; is a pretty broad term that includes lots of different algorithms, it is quite imprecise to say that animals have &ldquo;it&rdquo;.<br />It is quite clear that animals (including humans) use some form of RL, and also quite clear that they don&rsquo;t use all RL algorithms that some random computer scientist ever came up with.<br />Which forms of RL animals use exactly is still being debated and not conclusively established, but let&rsquo;s have a look at what science has uncovered!<br />One paradigm of RL that seems to play a big role in animals is called &ldquo;Temporal Difference Learning&rdquo;, or short: TD learning. Ever heard of the neurotransmitter &ldquo;dopamine&rdquo;, which is often colloquially called the &ldquo;pleasure hormone&rdquo; or the like? It is true that dopamine is, among other things, related to pleasure (reward!). But it seems that this is not the whole story; it is now believed that dopamine represents the anticipation of reward, or more precisely, it represents the prediction error of the reward. What the hell does that mean?<br />It is actually quite simple: the brain has expectations about the amount of reward a certain action will bring. The difference between this predicted reward and the actual outcome is called the prediction error. If there&rsquo;s more reward than expected, certain neurons show increased activity, and more dopamine than usual is emitted, which leads to a better prediction next time - learning has occured. This means describing dopamine as a simple reward neurotransmitter would be wrong - there is no dopamine reaction even in cases of very high reward, as long as the reward was already anticipated.</p>
            <p>(Actually, it is not the difference in reward that&rsquo;s computed, but rather the difference in state utilities (between expected utility and newly computed utility) - more on that later!)</p>
            
    </div>

    <div class="parallax-2"></div>

    <div class="parallax_text">
        That sounds more like psychology than computer science? That’s because it is! <br>
        Since it works so well in nature, computer science has adopted it as a Machine Learning (ML) technique, with huge success.
        As a consequence, RL is now a hot topic within the fields of computer science and psychology, and progress in either context can often be applied to the other field. This means new algorithms or insights that stem from computer science often help to deepen our understanding of RL in animals. On the other hand, the more we find out about how RL is implemented in the brain, the closer we get to building <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence"> artificial general intelligence (AGI)</a>, an AI that is on par with humans in cognitive abilities and might even surpass them soon thereafter. <br>
        Is that a goal we should work on? People disagree on this, but Google's <a href="https://www.technologyreview.com/s/601139/how-google-plans-to-solve-artificial-intelligence/">DeepMind certainly thinks so.</a>
        
    </div>

    <div class="parallax-2"></div>


    <div class="parallax_text">

        I said earlier that you learned to perform the optimal action for each state. In this case, this is true, the optimal action is always clicking on the circle. <br>
        But what if, by clicking on a square 20x in a row (getting punished each time), you would have been redirected to a hidden page that gives you a 20€ Amazon voucher? Obviously, this would have been worth the pain of getting a red-screen-punishment 20 times. <br>
        So maybe, you should have tried clicking on the square 20 times. But what if the threshold was at 30? Or 40? When should you stop smashing that square button like a mad person, performing the same action over and over again, expecting a different result? <br>
    </div>

    <div class="parallax"></div>

    <div class= "parallax_text">
        There are two important ideas that play a role here.
        
            <h3>The trade-off between exploration and exploitation</h3>    
        
        is the first one.
        When you learn something new, you first want to try out many different actions to find out which ones are the most rewarding (exploration). You are not satisfied with the first action that yields a positive reward, because there might be even better actions! But at some point, you want to use what you’ve learned so far and choose the actions that gave you the highest reward in the past (exploitation). The question is: how much should you explore, and when should you start exploiting your knowledge? Should you stop exploring completely at some point or still try out new things once in a while? If you don’t know the general answer, it’s because there is none. The right approach depends on the circumstances, and is in many cases simply not known.
        In most cases it's probably a good idea to start with a lot of exploration, and gradually shift the strategy towards more exploitation, only maintaining a very small amount of exploration after a lot of learning has already taken place.
        <br><br>
        <nav class="btn-bar nav-dark">

            <a class="btn btn-glass btn-info" onclick="reveal(this)" class="expand_button" id="1">
                <i class="fa fa-fw fa-lg fa-info " ></i> Learn more
            </a>
        </nav>
        
        
    </div>

    <div class="overlay" id="overlay1">
            <p>Selecting actions in a way that always exploits current knowledge is called greedy. &ldquo;Greedy action selection always exploits current knowledge to maximize immediate<br />reward; it spends no time at all sampling apparently inferior actions to see if they might really be better.&rdquo; [1]<br />Of course with this method, the algorithm can&rsquo;t learn very well. A simple way to exploit knowledge a lot, but which also guarantees we will also discover the best actions in the long run, is the &epsilon;-greedy method: it is greedy most of the time, but with a small probability &epsilon;, it selects a random action. For example, &epsilon;-greedy action selection with &epsilon; = 0.1 selects the best action, according to our current knowledge, 90% of the time, and in 10% of the cases it chooses a random action.</p>
            <p>So what&rsquo;s a good &epsilon; value? This depends on the circumstances, but here are some reflections. An &epsilon; value of 0 (meaning a greedy method) performs better in the short run, but might perform worse in the long run (by getting stuck in local maxima), because it never explores seemingly bad options. A big &epsilon; finds better actions quickly, but keeps on performing random (suboptimal) actions with the same big &epsilon; probability. A small &epsilon; on the other hand improves more slowly, because it doesn&rsquo;t try out new things that much, but eventually it will surpass the other two because it finds the optimal actions and only rarely chooses a random action, selecting the best actions almost always after a lot of training. [1]<br />As a compromise, one can use a shrinking &epsilon; value that starts out big and approaches 0 over time. In a static environment, this algorithm would learn quickly in the beginning, and transition to exploiting this knowledge more and more.</p>
            <p>This being said, an &epsilon; value that never goes under a certain threshold is still better suited for a changing environment where the optimal actions change over time.</p>
            
            <p class="reference">[1] R Sutton, A. Barto. 2017. Reinforcement Learning: An Introduction ****Complete Draft****
            p. 21-22</p>
    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        The other idea that plays a role here is the time frame over which you try to maximize the reward. Do you only care about the reward of the next action, or about the reward sum of all future actions?
        <br>

        The latter seems sensible, until you realize that the future reward might be infinite and you can’t calculate the best action anymore.
        <br>
        <br>
        In practice, most algorithms strike a balance here, and count reward that’s far away in the future much less than the reward of the next action. This is called
        <h3>discounting</h3>

        <nav class="btn-bar nav-dark">

                <a class="btn btn-glass btn-info" onclick="reveal(this)" class="expand_button" id="1.2">
                    <i class="fa fa-fw fa-lg fa-info " ></i> Notation
                </a>
        </nav>
    </div>

    <div class="overlay" id="overlay1.2">
            <p>The discount rate is written as gamma:</p>
            <p>&gamma; := discount rate<br />t := time step</p>
            <p>Example: Rt = rt + &gamma;*rt+1 + &gamma;&sup2;*rt+2 ...</p>
    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        This means ideally, we don't just choose the action that gives the highest reward now, we choose the one with the potentially highest (discounted)
        sum of rewards in the future. <br>
        In other words, if we see the future as different paths and the actions are the choices we make at the forks that lie ahead, we always want to take the fork that gives us access to the path that contains the most reward points, IF we decide optimally at all following forks.<br>
        
        This sum of potential future rewards is called "value" or
        <h3>utility</h3>
        Here, I will use the term "utility", because it is less ambiguous than "value".<br>
        Above, I said "ideally", because usually we don't know for certain which action has the highest potential for future reward - but we might have an expectation. This expectation is called
        <h3>expected utility</h3>
        
    </div>

    

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        As I said earlier, there are basically three variables in RL we need to consider: state, action and reward.
Ideally, the <h3>state</h3> contains complete information about the environment we’re in, but most of the time 
we can’t fully know everything that’s going on. So more realistically, the state contains all the information that we know and that we also want to use for our  decisions. For example in a chess game, it would describe the position of all the pieces, but for a self-driving car, the state would not contain all the positions of every single atom in the world, it would consist of all the sensory input, which is of course just an approximation of the real world.
    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        Now, when we only have a small set of states, we can “simply” calculate the rewards of each state and we have a working solution: in each state, choose the action that has the highest utility.
        <br><br>
        <nav class="btn-bar nav-dark">

            <a class="btn btn-glass btn-info" onclick="reveal(this)" class="expand_button" id="1.1">
                <i class="fa fa-fw fa-lg fa-info " ></i> Learn more
            </a>
    </nav>

    </div>

    <div class="overlay" id="overlay1.1">
            <p>The equation that describes the optimal actions with regard to their future reward potential is known as the &ldquo;Bellman optimality equation&rdquo;.</p>
            <p>Let&rsquo;s define the function that gives us the utility u (the sum of future rewards) of a state s. The utility would of course depend on the actions that the agent takes in the future. The book of rules that guides the behaviour of an agent is called a policy, denoted as 𝜋.<br />Then the function would look like this<br />u<sub>𝜋</sub>(s)<br />Meaning: the utility u of a state s when following the policy 𝜋.<br />Earlier I said we want to take the fork that gives us access to the path with the most utility, under the premise that we decide optimally at all following forks. The function that returns the utility of a state under an optimal policy 𝜋<sub>*</sub> (meaning the agents makes optimal decisions) is written like this<br />u<sub>*</sub>(s)<br />Now, if we want to know which path to take, we&rsquo;d have to look up where the paths lead and calculate the utility of all the states that we can reach ( u<sub>*</sub>(s1), u<sub>*</sub>(s2), u<sub>*</sub>(s3), &hellip;).<br />It gets even easier when we use a function that gives us the utility of a certain action that we can choose when we are in a specific state. In order to not confuse this function with the other function, let&rsquo;s denote the utility function as q this time.<br />So let&rsquo;s define the function that gives us the utility of an action a, when we&rsquo;re in state s, under an optimal policy, as<br />q<sub>*</sub>(s, a)<br />This function returns the utility of an action that we decide on in a specific state (the same action might yield different results in different states).<br />Okay, so we defined a function, but how does the term for computing the utility actually look like?</p>

            <img src="pictures/equations/bellman.svg" title="This is the optimal equation. You may not like it, but this is what peak performance looks like." alt="The Bellman equation for q(s, a)" style="padding:50px; width: 80%;">

            <p>Or, when expressed in regard to a state-action-pair:</p>
            
            <img src="pictures/equations/bellman_q.svg" title="This is the optimal equation. You may not like it, but this is what peak performance looks like." alt="The Bellman equation for q(s, a)" style="padding:50px; width: 80%;">

            <p>This is the Bellman optimality equation.<br /> u<sub>*</sub>(s) is interpreted as such:<br />Of the available actions in state s, choose the one with the maximum sum that is comprised of the probability of a reward, times the reward plus the utility of the <i>next</i> state. The utility of the next state is discounted by the discount rate &gamma;.<br />As you can see, it is recursive: u<sub>*</sub>(s) contains u<sub>*</sub>(s&rsquo;), where s&rsquo; is the next state. This means writing the whole equation down for an actual RL problem would be infeasible for a human, but it&rsquo;s standard practice for a computer, and makes this a part of dynamic programming.</p>
            <p>This is the theoretically ideal solution for an environment of which the dynamics are known. So did we already &ldquo;solve&rdquo; reinforcement learning for environments where we have complete information? Unfortunately, not at all. As you might already suspect, since it is recursive, the Bellman optimality equation takes unfathomable amounts of computing power and memory in complex environments, which makes it infeasible for almost all real world applications. Additionally, the most interesting problems which we want so solve with ML often have incomplete information, meaning the dynamics of the environment are not fully known. Still, the Bellman optimality equation is important groundwork, and many RL algorithms build on it by trying to approximate its solution [1].</p>
            <p class="reference">[1] https://www.tu-chemnitz.de/informatik/KI/scripts/ws0910/ml09_3.pdf - p.25/slide 48</p>
                        
    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        It is important to realize though that in most real world tasks, a nearly infinite amount of states exists, mostly because a state is defined by a combination of parameters, which leads to a combinatorial explosion.
        <br><br>
        <nav class="btn-bar nav-dark">
            <a class="btn btn-glass btn-warning" onclick="reveal(this)" class="expand_button" id="2">
                <i class="fa fa-fw fa-lg fa-exclamation" ></i> Example
            </a>
        </nav>
        
    </div>

    <div class="overlay" id="overlay2">
            <p>Think of an autonomous car: the number of different speed-states isn&rsquo;t that high. Even if you don&rsquo;t round, and the speed parameter has a precision of 3 decimal places (like 34.585 km/h), there are only about 150&rsquo;000 different states - that&rsquo;s nothing for a computer. The car could simply learn the optimal action for each of the 150&rsquo;000 states, like &ldquo;break with a force of 5.0&rdquo; at the speed of 148.999 km/h, or &ldquo;speed up with a force of 3.1&rdquo; at the speed of 5.122 km/h, right?<br />Except of course the parameter of speed alone is useless, the decision to break or speed up can&rsquo;t be made solely by knowing how fast the car is going. You need to consider a lot of parameters in combination, like distance to car in front, distance to car behind, steepness of the curve, probability of pedestrians jumping on the street etc. But if you have 150&rsquo;000 different possible values for each parameter, and just 10 parameters (you&rsquo;d probably have many more), this suddenly creates 5.76650390625&times;10<sup>51</sup> distinct states - that&rsquo;s about 6 sexdecillion states. Even if you rounded all parameters to only 150 different values per parameter, you&rsquo;d still have around 10<sup>21</sup> states - a one-petabyte hard drive only has 10<sup>15</sup> bytes, so you would need at least 1 million of them just to save all the possible states, and then you&rsquo;d still need to figure out a policy to navigate through that state-space.</p>

    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        So in order to train an algorithm in a complex environment, like a self-driving car, it’s not possible for it to consider every single possible state.
Now you might say “and that’s not necessary at all, there are a lot of states that are basically the same, so by figuring out the best action for one state, it already tells you how to behave in a lot of different states. If I learn to stop when a pedestrian is 10 meters in front of the car, to avoid the negative reward of hitting someone with a car, it’s obvious that I should also stop the car if the pedestrian is 9.999 or 10.001 meters away!”
And you’re right, but it’s important to realize that this is not at all obvious to our algorithm. The main reason for that is that our algorithm knows basically nothing about the world. It doesn’t know what a pedestrian is or that 9.999 meters distance can be judged the same way as 10 meters. In fact, “if pedestrian 10 meters in front, then stop” isn’t even a sensible rule - if the street is bending away from the pedestrian anyway, we don’t need to slow down. But our algorithm doesn’t know the concept of a curve, and cannot consider its implications.
    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        In this light it’s astonishing how successful reinforcement learning is, despite this huge problem of the combinatorial explosion and the lack of an accurate world model of the algorithms. How was this achieved?

    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        Okay, so it would be great if our algorithm learned to recognize similar states, even if it never encountered the particular state it’s now in.
        This is a big part of reinforcement learning, and it’s called
        <h3>function approximation</h3>
        Imagine you have a function where you put in a state, and it gives back to you the true utility of that state. You could use it to calculate the utility of the next possible states that you can reach from the state you’re in, and you would then choose the action that brings you to the state with the highest utility. That function exists somewhere, even if we don’t know how it looks, and if we assume some regularity (meaning that there is a kind of pattern in the relationship between states and utility), knowing only a few points of the graph (tuples of parameters and utility) should tell us something about the rest of the graph. Of course, the more points we know, the better we can approximate the function.
        <br><br>
        <nav class="btn-bar nav-dark">
            <a class="btn btn-glass btn-warning" onclick="reveal(this)" class="expand_button" id="3">
                <i class="fa fa-fw fa-lg fa-exclamation" ></i> Example
            </a>
        </nav>
    </div>

    <div class="overlay" id="overlay3">
        Let’s say we have 4 data points, in the form of states and their respective utility (of course we don’t know the true utility, and that’s also a problem, but we can approximate their utility based on the rewards we got so far).
        <br>
        Which function describes all 4 data points?
        <br>
        <img src="pictures/function_approximation1.png" title="function approximation graph 1" alt="4 unconnected points in a graph: high low low high" style="padding:50px; width: 80%;">
        <br>
        A parabola!
        <br>
        <img src="pictures/function_approximation2.png" title="function approximation graph 2" alt="4 connected points in a graph: high low low high" style="padding:50px; width: 80%;">
        <br>
        So based on the data we got, we might assume that our utility function is something like
        <br>Utility(state) = state<sup>2</sup><br>
        Based on that, we can calculate the utilities of states which we never encountered in our training (and therefore don’t know the long-term expected reward) and decide how we should act in an unknown situation! Of course, with so little data, our approximation is likely to be quite erroneous.
        Have a look at how the same set of points can be described by different functions:
        <br>
        <img src="pictures/function_approximation3.png" title="function approximation graph 3" alt="4 points in a graph that are connected in 2 different ways" style="padding:50px; width: 80%;">
        <br>
For example, if the x-axis was describing a car’s position on a 3 lane highway, we might have had good experiences with driving on the right and driving on the left. The data points in between have low utility, because they describe the position between the left/right lane and the center lane - driving between lanes is not rewarded highly! But we don’t realize that the actual function has a spike in the middle, because driving on the middle lane is also okay.

This example was of course simplified to the extreme; in reality, the function has a lot more than two dimensions, about a million or more, and it’s much more difficult to find a function that describes the available data points.

One particular form of function approximation that you have probably heard of are “Neural Networks”. The combination of RL and NN is extremely powerful and has yielded results like the groundbreaking “Alpha Go” by Google. Just remember that NNs themselves are not goal-oriented, that’s where RL comes into play!
And although NNs are very popular, they are not the only powerful form of function approximation.

    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">
        Alright, to wrap it all up I prepared a short quiz for you.

        <form>
            
            <fieldset>
                <p>Which is another word for utility in the context of RL?</p>

                <input type="checkbox" id="toggle-1">
                
                
                
                <label class="container" for="answer1.1"> value - the sum of the expected rewards 
                    <input type="radio" id="answer1.1" name="Utility" value="value - the sum of the expected rewards">
                    <span class="checkmark"></span>                
                </label>
                
                
                
                <label class="container" for="answer1.2"> reward - the immediate payoff for the agent
                    <input type="radio" id="answer1.2" name="Utility" value="reward - the immediate payoff for the agent">
                    <span class="checkmark"></span>
                </label>

                
                
                <label class="container" for="answer1.3"> environment - the world which the agent explores
                    <input type="radio" id="answer1.3" name="Utility" value="environment - the world which the agent explores">
                    <span class="checkmark"></span>
                </label>

                
                
                <label class="container" for="answer1.4"> happiness - the aggregated sum of happiness minus suffering
                    <input type="radio" id="answer1.4" name="Utility" value="happiness - the aggregated sum of happiness minus suffering">
                    <span class="checkmark"></span>
                </label>
                <p></p>
                <label for="toggle-1" class="checkanswer">Check answer</label>
            </fieldset>

            <fieldset>
                <p>What does the epsilon in ε-greedy stand for?</p>

                <label class="container" for="answer2.1"> the number of exploring actions before the greedy policy kicks in 
                    <input type="radio" id="answer2.1" name="Greedy" value="the number of exploring actions before the greedy policy kicks in">
                    <span class="checkmark"></span>                
                </label>
                
                

                <input type="checkbox" id="toggle-2">
                
                <label class="container" for="answer2.3"> the probability of diverging from greedy behaviour
                    <input type="radio" id="answer2.3" name="Greedy" value="the probability of diverging from greedy behaviour">
                    <span class="checkmark"></span>
                </label>

                
                
                <label class="container" for="answer2.4"> the value that the greedy policy optimizes for
                    <input type="radio" id="answer2.4" name="Greedy" value="the value that the greedy policy optimizes for">
                    <span class="checkmark"></span>
                </label>
                <p></p>
                <label for="toggle-2" class="checkanswer">Check answer</label>
            </fieldset>

<!--       The following question asks for knowledge that isnt provided in the texts

            <fieldset>
                <p>What is needed for Supervised Learning to work?</p>

                <input type="checkbox" id="toggle-3">

                <label class="container" for="answer3.1"> labelled data 
                    <input type="radio" id="answer3.1" name="SupervisedLearning" value="labelled data">
                    <span class="checkmark"></span>                
                </label>
                
                
                
                <label class="container" for="answer3.3"> a fitting reward signal
                    <input type="radio" id="answer3.3" name="SupervisedLearning" value="a fitting reward signal">
                    <span class="checkmark"></span>
                </label>

                
                
                <label class="container" for="answer3.4"> any data that exhibits some form of structure
                    <input type="radio" id="answer3.4" name="SupervisedLearning" value="any data that exhibits some form of structure">
                    <span class="checkmark"></span>
                </label>
                <p></p>
                <label for="toggle-3" class="checkanswer">Check answer</label>
            </fieldset> -->

            <fieldset>
                <p>Why is discounting of rewards not just helpful, but often necessary?</p>

                
                
                <label class="container" for="answer4.1"> because rewards would otherwise dominate the calculation 
                    <input type="radio" id="answer4.1" name="Discounting" value="because rewards would otherwise dominate the calculation">
                    <span class="checkmark"></span>                
                </label>
                
                <input type="checkbox" id="toggle-4">
                
                <label class="container" for="answer4.3"> to allow calculations when dealing with infinities
                    <input type="radio" id="answer4.3" name="Discounting" value="to allow calculations when dealing with infinities">
                    <span class="checkmark"></span>
                </label>

                
                
                <label class="container" for="answer4.4"> to create a gradient descent that the algorithm can follow
                    <input type="radio" id="answer4.4" name="Discounting" value="to create a gradient descent that the algorithm can follow">
                    <span class="checkmark"></span>
                </label>
                <p></p>
                <label for="toggle-4" class="checkanswer">Check answer</label>
            </fieldset>

            
        </form>

        
    </div>

    <div class="parallax">
    </div>

    <div class= "parallax_text">

            <h3><span title="...and they lived happily ever after. The AIs, that is. Humans went extinct.">the end</span></h3>
            And that’s it! You now know about the key concepts and problems in reinforcement learning. Congratulations! <br><br>

            For some hands on experience, I created a Jupyter Notebook that presents and explains an RL-algorithm: policy iteration. You can execute the code to solve an actual problem and even modify it if you want to! To initialize the notebook with binder, klick on this button:
            <a href="https://mybinder.org/v2/gh/Favodar/Reinforcement-Learning/master?filepath=Jacks-Car-Rental%2FPolicyIterationNotebook.ipynb">
                <img src="https://mybinder.org/badge_logo.svg" alt="launch binder">
            </a>
            <br>
            You may view and download all the files on <a href="https://github.com/Favodar/Reinforcement-Learning">GitHub</a>.
            <br><br>

            If you would like to learn a lot more from the pioneers of reinforcement learning themselves, go check out <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf"> this freely available and extraordinarily well written book</a> by Richard Sutton and Andrew Barto. <br><br>

            Or you could sign up for a <a href="https://en.wikipedia.org/wiki/Massive_open_online_course">MOOC</a> if you prefer a university-style seminar. There are many different MOOCs on the topic out there. For folks with a degree in computer science (or similar), I can recommend <a href="https://eu.udacity.com/course/reinforcement-learning--ud600"> this course </a> by Georgia Tech.
            <br><br>

            <nav class="btn-bar nav-dark">
            <a class="btn btn-glass btn-info" onclick="reveal(this)" class="expand_button" id="4">
                    <i class="fa fa-fw fa-lg fa-info " ></i> About the author
                </a>
            </nav>
    </div>

    <div class="overlay" id="overlay4">
        Fritz Dorn studies media computer science at the HSD - Hochschule Düsseldorf. His subjects of interest are virtual reality, machine learning and IT security.
        This website is a project supervised by professor Christian Geiger.
        <br><br>
        Square and circle puzzle created by <a href="https://github.com/cyluxx">Andreas Plewnia</a> for this introduction.
    </div>



    </div>
    </div>

</body>

<script src="js/parallax.js"></script>

<!-- Parallax libraries (not in use as of now, were intended to fix parallax for mobile): -->
<script src="js/rellax.js"></script>
<!-- <script>
  // Accepts any class name
  var rellax = new Rellax('.rellax');
</script> -->
<!-- <script src="parallax_lib.js"></script> -->


</html>